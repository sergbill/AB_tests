{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# базовые библиотеки\n",
    "from sklearn.model_selection import cross_validate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "from tqdm import tqdm_notebook\n",
    "from scipy.sparse import hstack, vstack, csc_matrix\n",
    "import os, re, sys, gc, pickle, time\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "# валидация, оптимизация гиперпараметров\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score,\\\n",
    "                                    KFold, train_test_split, cross_validate, ParameterGrid\n",
    "\n",
    "# пайплайн\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin,  clone\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, OneHotEncoder\n",
    "\n",
    "# дамми-регрессор\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import time\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_types(df):\n",
    "    '''конвертирует типы (по возможности)'''\n",
    "    df_c = df.copy()\n",
    "    for col in df.columns:\n",
    "        ser = df[col]\n",
    "        try:\n",
    "            ser2 = ser.astype('datetime64')\n",
    "            df_c[col] = ser2\n",
    "        except:\n",
    "            try:\n",
    "                ser2 =ser.astype(int)\n",
    "                if (ser != ser2).any():\n",
    "                    try:\n",
    "                        df_c[col] =ser.astype(float)\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "    try:\n",
    "        return pd.concat([df_c.select_dtypes('datetime64'), df_c.select_dtypes(exclude = ['datetime64'])], 1)\n",
    "    except:\n",
    "        return df_c\n",
    "    \n",
    "def profile_df(df):\n",
    "    '''отчет по датафрейму'''\n",
    "    df_c = df.copy()\n",
    "    _row_duplicates = df.duplicated().mean()\n",
    "    _types = df.dtypes\n",
    "    _nan_mean = df.isna().mean()\n",
    "    _nunique = df.nunique() / df.shape[0]\n",
    "    \n",
    "    report_df = pd.concat([_types, _nan_mean, _nunique], 1)\n",
    "    report_df.columns = ['тип','доля_nan', 'доля_уник_зн']\n",
    "    report_df.index = df.columns\n",
    "    print('% дубликатов строк равен {:.5%}'.format(_row_duplicates))\n",
    "    \n",
    "    # удаляем дубликаты строк\n",
    "    df_c = df_c[~df.duplicated()].reset_index(drop = True)\n",
    "    # удаляем колонки с 1 уникальным значением\n",
    "    df_c = df_c.loc[:, df_c.fillna('nan').nunique()!=1]\n",
    "    return (df_c, report_df)\n",
    "\n",
    "def train_hold_test_split(features, target, tr_size, ho_size, shuffle, random_state, stratify, use_test):\n",
    "    if use_test:\n",
    "        # делим данные на тренировочную, отложенную, тестовую части\n",
    "        features_train, features_te, target_train, target_te = train_test_split(\\\n",
    "                                                                       features, target,\\\n",
    "                                                                       train_size = tr_size,\\\n",
    "                                                                       shuffle = shuffle, random_state = random_state,\\\n",
    "                                                                       stratify = stratify)\n",
    "        features_tr, features_ho, target_tr, target_ho = train_test_split(\\\n",
    "                                                                       features_train, target_train,\\\n",
    "                                                                       train_size = 1-ho_size,\\\n",
    "                                                                       shuffle = shuffle, random_state = random_state,\\\n",
    "                                                                       stratify = stratify)\n",
    "        print('train size = {}, hold size ={}, test size = {}'\\\n",
    "              .format(features_tr.shape[0], features_ho.shape[0], features_te.shape[0]))\n",
    "        return (features_tr, features_ho, features_te, target_tr, target_ho, target_te)\n",
    "    else:\n",
    "        # делим данные на тренировочную, отложенную, тестовую части\n",
    "        features_tr, features_ho, target_tr, target_ho = train_test_split(\\\n",
    "                                                             features, target,\\\n",
    "                                                             train_size = tr_size,\\\n",
    "                                                             shuffle = shuffle, random_state = random_state,\\\n",
    "                                                             stratify = stratify)\n",
    "        print('train size = {}, hold size ={}'\\\n",
    "              .format(features_tr.shape[0], features_ho.shape[0]))\n",
    "        return (features_tr, features_ho, target_tr, target_ho)\n",
    "    \n",
    "class SklearnHelperColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    '''выбирает колонки, отпавляемые в пайплайн'''\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.columns]\n",
    "\n",
    "class SklearnHelperLabelEncoder(TransformerMixin, BaseEstimator):\n",
    "    ''' Факторизация категорий '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        X_c = X.astype(str)\n",
    "        self.d1 = {}\n",
    "        for col in X_c.columns:\n",
    "            uniques = X_c[col].dropna().unique() \n",
    "            self.d1[col] =  dict(zip(uniques, range(len(uniques))))              \n",
    "        return self\n",
    "    def transform(self, X): \n",
    "        X_c = X.astype(str)\n",
    "        for key, value in self.d1.items():\n",
    "            X_c[key] = X_c[key].map(value)\n",
    "        return X_c\n",
    "\n",
    "class SklearnHelperTargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    ''' Кодирование категорий с помощью целевой переменной '''\n",
    "    def __init__(self, n_iter, n_folds, min_samples_leaf, seed):\n",
    "        self.n_iter = n_iter\n",
    "        self.n_folds = n_folds\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.seed = seed\n",
    "    def fit(self, X, y=None):\n",
    "        self.y_mean = y.mean()\n",
    "        _df_tr = pd.concat([X, y], 1)\n",
    "        target_col = _df_tr.columns[-1]\n",
    "        to_encode = _df_tr.columns[:-1]\n",
    "        \n",
    "        L_tr = []        \n",
    "        self.L_d_encs = []\n",
    "        for i in tqdm_notebook(range(self.n_iter)): \n",
    "            enc_tr = pd.DataFrame(index = _df_tr.index, columns = to_encode).fillna(0.0)\n",
    "            for col in to_encode:\n",
    "                for tr_idx, val_idx in KFold(self.n_folds, shuffle = True,random_state = self.seed+i)\\\n",
    "                                       .split(_df_tr):                    \n",
    "                    grp = _df_tr.iloc[tr_idx].groupby(col)[target_col].agg({'mean', 'count'})                    \n",
    "                    d_enc = grp[grp['count']>=self.min_samples_leaf]['mean'].to_dict()\n",
    "                    self.L_d_encs.append((col, d_enc))\n",
    "                    to_enc_tr =_df_tr.iloc[val_idx]                    \n",
    "                    enc_tr.loc[to_enc_tr.index, col] = to_enc_tr[col].map(d_enc).fillna(_df_tr.iloc[tr_idx][target_col].mean())                   \n",
    "            L_tr.append(enc_tr)    \n",
    "            \n",
    "        self.enc_tr =  pd.concat(L_tr, 1)\n",
    "        self._df_tr = _df_tr\n",
    "        return self    \n",
    "    def transform(self, X):\n",
    "        if np.all(X.values == self._df_tr.values):\n",
    "            return self.enc_tr\n",
    "        else:\n",
    "            df_enc = pd.DataFrame(index = X.index, columns=X.columns).fillna(0.0)\n",
    "            for feat, d in tqdm_notebook(self.L_d_encs):\n",
    "                df_enc.loc[:, feat] += X[feat].map(d) / self.n_iter\n",
    "            return df_enc\n",
    "\n",
    "class SklearnHelperFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    ''' Отбор признаков '''\n",
    "    def __init__(self, model, cv, scoring, show_progress):\n",
    "        self.model = model\n",
    "        self.cv = cv\n",
    "        self.scoring = scoring\n",
    "        self.show_progress = show_progress\n",
    "    def fit(self, X, y=None):\n",
    "        try:\n",
    "            _X = X.todense()\n",
    "        except:\n",
    "            _X =X.copy()            \n",
    "        cv_scores = []\n",
    "        for i in tqdm_notebook(range(_X.shape[1])):\n",
    "            _X_curr = _X[:, i].reshape(-1,1)\n",
    "            mean_cv_score = cross_val_score(self.model, _X_curr, y, cv =self.cv, scoring = self.scoring, n_jobs=-1).mean()\n",
    "            \n",
    "            cv_scores.append(mean_cv_score)\n",
    "        order = np.argsort(cv_scores)[::-1]\n",
    "        to_drop_before, best_features, best_cv_score = [], [], -np.inf\n",
    "        for i in tqdm_notebook(order):\n",
    "            curr_features = best_features+[i]\n",
    "            _X_curr = _X[:, curr_features]\n",
    "            mean_cv_score = cross_val_score(self.model, _X_curr, y, cv =self.cv, scoring = self.scoring, n_jobs=-1).mean()\n",
    "            if mean_cv_score>best_cv_score:\n",
    "                best_cv_score = mean_cv_score\n",
    "                best_features = curr_features\n",
    "                if self.show_progress:\n",
    "                    print('new best score = {:.5f}'.format(best_cv_score))\n",
    "            else:\n",
    "                to_drop_before.append(i)\n",
    "        while True:\n",
    "            to_drop_after = []\n",
    "            for i in tqdm_notebook(to_drop_before):\n",
    "                curr_features = best_features+[i]\n",
    "                _X_curr = _X[:, curr_features]\n",
    "                mean_cv_score = cross_val_score(self.model, _X_curr, y, cv =self.cv, scoring = self.scoring, n_jobs=-1).mean()\n",
    "                if mean_cv_score>best_cv_score:\n",
    "                    best_cv_score = mean_cv_score\n",
    "                    best_features = curr_features\n",
    "                    if self.show_progress:\n",
    "                        print('new best score = {:.5f}'.format(best_cv_score))\n",
    "                else:\n",
    "                    to_drop_after.append(i)\n",
    "            if to_drop_before == to_drop_after:\n",
    "                break\n",
    "            else:\n",
    "                to_drop_before = to_drop_after  \n",
    "        self.best_features = best_features\n",
    "        self.best_cv_score = best_cv_score\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, csc_matrix):\n",
    "            _X = X.copy()\n",
    "        else:            \n",
    "            _X = csc_matrix(X) \n",
    "        return _X[:, self.best_features]\n",
    "    \n",
    "def get_classic_stacked_metafeatures(base_models,\\\n",
    "                                     base_features_train, base_features_hold, base_features_test,\\\n",
    "                                     y_train, y_hold, y_test,\\\n",
    "                                     n_folds,\\\n",
    "                                     seed):\n",
    "    ''' \n",
    "    Каждая базовая модель обучается на всем пространстве признаков с помощью к-фолд валидации\n",
    "    - ооф предсказания формируют метапризнаки\n",
    "    - метапризнаки для отложенной и тестовой частей получаются усреднением предсказаний обученных на валидации моделей\n",
    "    '''\n",
    "    # списки с метапризнаками\n",
    "    L_meta_tr, L_meta_hold, L_meta_te = [], [], []\n",
    "    \n",
    "    # фиксируем валидацию\n",
    "    stack_kf = KFold(n_folds, random_state = seed, shuffle = True)\n",
    "    \n",
    "    # проходим по моделям\n",
    "    for i in tqdm_notebook(range(len(base_models))):\n",
    "        \n",
    "        # выбираем модель, признаки\n",
    "        _model = base_models[i]\n",
    "        _X_train = base_features_train[i]\n",
    "        _X_hold = base_features_hold[i]\n",
    "        _X_test = base_features_test[i]\n",
    "            \n",
    "        Z_train = np.zeros((_X_train.shape[0], 1))\n",
    "        Z_hold = np.zeros((_X_hold.shape[0], n_folds))\n",
    "        Z_test = np.zeros((_X_test.shape[0], n_folds))\n",
    "        \n",
    "        # делаем ооф предсказания\n",
    "        for j, (tr_idx, val_idx) in enumerate(stack_kf.split(y_train)):\n",
    "            _model.fit(_X_train[tr_idx], y_train[tr_idx])\n",
    "            Z_train[val_idx, 0] = _model.predict(_X_train[val_idx])\n",
    "            Z_hold[:, j] = _model.predict(_X_hold)\n",
    "            Z_test[:, j] = _model.predict(_X_test)\n",
    "        \n",
    "        # для отложенной и теста усредняем ооф предсказания, сохраняем\n",
    "        L_meta_tr.append(Z_train)\n",
    "        L_meta_hold.append(np.mean(Z_hold, 1))\n",
    "        L_meta_te.append(np.mean(Z_test, 1))\n",
    "\n",
    "    X_meta_tr = np.column_stack(L_meta_tr)\n",
    "    X_meta_ho = np.column_stack(L_meta_hold)\n",
    "    X_meta_te = np.column_stack(L_meta_te)\n",
    "    \n",
    "    return (X_meta_tr, X_meta_ho, X_meta_te)\n",
    "\n",
    "def get_stacked_random_subsample_metafeatures(base_models,\\\n",
    "                                              base_features_train, base_features_hold, base_features_test,\\\n",
    "                                              y_train, y_hold, y_test,\\\n",
    "                                              n_iterations,\\\n",
    "                                              n_folds,\\\n",
    "                                              seed):\n",
    "    '''     \n",
    "    Каждая базовая модель обучается на всем пространстве признаков с помощью к-фолд валидации\n",
    "    - ооф предсказания формируют метапризнаки\n",
    "    - метапризнаки для отложенной и тестовой частей получаются усреднением предсказаний обученных на валидации моделей\n",
    "    \n",
    "    upd: n_iterations выбирается модель, для модели выбирается доля исходных признаков (subsample) (от .5 до .9),\\\n",
    "    которые участвуют в обучении, на каждой итерации используется новая схема валидации\n",
    "    '''\n",
    "    # размеры подвыборок\n",
    "    subsamples = [.5, .6, .7, .8, .9]\n",
    "    # индексы моделей\n",
    "    indexes = np.arange(len(base_models)) \n",
    "    # метапризнаки\n",
    "    L_X_meta_tr, L_X_meta_ho, L_X_meta_te = [], [], []\n",
    "    \n",
    "    # итерируемся\n",
    "    for i in tqdm_notebook(range(n_iterations)):\n",
    "        \n",
    "        # фиксируем сид\n",
    "        _seed = i+seed\n",
    "        np.random.seed(_seed)\n",
    "        # фиксируем валидацию\n",
    "        kf = KFold(n_folds, shuffle=True, random_state = _seed)\n",
    "        # выбираем индекс\n",
    "        current_idx = np.random.choice(indexes)\n",
    "        # выбираем размер подвыборки\n",
    "        subsample = np.random.choice(subsamples)\n",
    "        \n",
    "        # выбираем модель\n",
    "        current_model = base_models[current_idx]\n",
    "        # выбираем признаки\n",
    "        X_tr = base_features_train[current_idx]\n",
    "        X_ho = base_features_hold[current_idx]\n",
    "        X_te = base_features_test[current_idx]\n",
    "        # выбираем подпространсво признаков\n",
    "        feat_subspace = np.random.choice(np.arange(X_tr.shape[1]),\\\n",
    "                                         np.int32(np.around(subsample*X_tr.shape[1])),\\\n",
    "                                         replace = False)\n",
    "        _X_tr = X_tr[:,feat_subspace]\n",
    "        _X_ho = X_ho[:,feat_subspace]\n",
    "        _X_te = X_te[:,feat_subspace]        \n",
    "        del X_tr, X_ho, X_te\n",
    "        gc.collect()\n",
    "        \n",
    "        Z_tr = np.zeros((_X_tr.shape[0], 1))\n",
    "        Z_ho = np.zeros((_X_ho.shape[0], n_folds))\n",
    "        Z_te = np.zeros((_X_te.shape[0], n_folds))\n",
    "        \n",
    "        # делаем ооф предсказания, предсказания для отложенной, для тестоой\n",
    "        for i, (tr_idx, val_idx) in enumerate(kf.split(_X_tr)):\n",
    "            current_model.fit(_X_tr[tr_idx], y_train[tr_idx])\n",
    "            Z_tr[val_idx, 0] = current_model.predict(_X_tr[val_idx])\n",
    "            Z_ho[:, i] = current_model.predict(_X_ho)\n",
    "            Z_te[:, i] = current_model.predict(_X_te)\n",
    "            \n",
    "        # сохраняем ооф предсказания\n",
    "        L_X_meta_tr.append(Z_tr)\n",
    "        # усредняем предсказания по фолдам для отложенной и тестовой частей\n",
    "        # сохраняем\n",
    "        L_X_meta_ho.append(np.mean(Z_ho, 1))\n",
    "        L_X_meta_te.append(np.mean(Z_te, 1))\n",
    "    \n",
    "    # собираем сохраненные предсказания     \n",
    "    X_meta_tr = np.column_stack(L_X_meta_tr)\n",
    "    X_meta_ho = np.column_stack(L_X_meta_ho)\n",
    "    X_meta_te = np.column_stack(L_X_meta_te)\n",
    "    \n",
    "    return (X_meta_tr, X_meta_ho, X_meta_te)    \n",
    "\n",
    "def hp_tune_v1(model, grid, X, y, cv, scoring):    \n",
    "    gs = GridSearchCV(model,param_grid=grid,cv = cv, scoring = scoring, n_jobs=-1, verbose = 1)\n",
    "    gs.fit(X, y)\n",
    "    best_estimator_ = clone(gs.best_estimator_)\n",
    "    del gs\n",
    "    gc.collect()    \n",
    "    return best_estimator_\n",
    "\n",
    "\n",
    "def hp_tune_v2(model, grid1, grid2, grid3, X_tr, y_tr, X_ho, y_ho, cv, scoring): \n",
    "    fit_params={'early_stopping_rounds':10,\\\n",
    "                'eval_set':[(X_ho, y_ho)],\\\n",
    "                'verbose':0}\n",
    "    gs = GridSearchCV(model, param_grid=grid1, cv = cv, scoring=scoring, n_jobs=-1, verbose=1)\n",
    "    gs.fit(X_tr, y_tr, **fit_params)    \n",
    "    bp = gs.best_params_\n",
    "    model = model.set_params(**bp)\n",
    "    del gs\n",
    "    gc.collect()\n",
    "\n",
    "    gs = GridSearchCV(model,param_grid = grid2, cv = cv, scoring = scoring, n_jobs=-1, verbose = 1)\n",
    "    gs.fit(X_tr, y_tr, **fit_params)    \n",
    "    bp.update(gs.best_params_)\n",
    "    model = model.set_params(**bp)\n",
    "    del gs\n",
    "    gc.collect()\n",
    "    bp_c = bp.copy()\n",
    "    \n",
    "    best_score = -np.inf\n",
    "    for params in tqdm_notebook(list(ParameterGrid(grid3))):\n",
    "        bp_c.update(params)\n",
    "        model = model.set_params(**bp_c)\n",
    "        mean_cv_score = cross_val_score(model, X_tr, y_tr, cv=cv, scoring =scoring, n_jobs=-1).mean()\n",
    "        if mean_cv_score>best_score:\n",
    "            best_score = mean_cv_score            \n",
    "            best_estimator_ = model\n",
    "        else:\n",
    "            break\n",
    "    return clone(best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# константы\n",
    "SEED = 13\n",
    "# валидация\n",
    "KF = KFold(3, random_state = SEED, shuffle = True)\n",
    "# метрика качества\n",
    "def neg_rmse_func(y_true, y_pred):\n",
    "    return -np.sqrt(np.mean((y_true-y_pred)**2))\n",
    "NEG_RMSE_SCORER = make_scorer(neg_rmse_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем данные, определяем типы \n",
    "df = convert_types(pd.read_csv('datasets/autos.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['DateCrawled', 'LastSeen']:\n",
    "    df[col+'.year'] = df[col].dt.year\n",
    "    df[col+'.month'] =df[col].dt.month\n",
    "    df[col+'.day'] =df[col].dt.day\n",
    "    df[col+'.dayofweek'] =df[col].dt.dayofweek\n",
    "    df[col+'.hour'] =df[col].dt.hour\n",
    "    df[col+'.minute'] =(df[col].dt.minute / 60).round()\n",
    "    df[col+'.dayofyear'] =df[col].dt.dayofyear\n",
    "    df[col+'.weekofyear'] =df[col].dt.weekofyear\n",
    "    df[col+'.quarter'] =df[col].dt.quarter    \n",
    "df['DateCreated.year'] = df['DateCreated'].dt.year\n",
    "df['DateCreated.month'] = df['DateCreated'].dt.month\n",
    "df['DateCreated.day'] = df['DateCreated'].dt.day\n",
    "df.drop(['DateCrawled', 'DateCreated', 'LastSeen'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size = 287038, hold size =31894, test size = 35437\n"
     ]
    }
   ],
   "source": [
    "# признаки, целевой признак\n",
    "features, target = df.drop('Price', 1),df['Price'] \n",
    "\n",
    "features_tr, features_ho, features_te, target_tr, target_ho, target_te = \\\n",
    "    train_hold_test_split(features, target, tr_size=.9, ho_size=.1,\\\n",
    "                          shuffle=True, random_state=SEED,\\\n",
    "                          stratify=None,  use_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuniques =features_tr.nunique()\n",
    "nuniques_nonone = nuniques[nuniques!=1].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_nuniques = nuniques_nonone<100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obj = features_tr.select_dtypes(object)\n",
    "df_num = features_tr.drop(df_obj.columns,1)\n",
    "num_nuniques = df_num.nunique()\n",
    "num_nuniques_nonones = num_nuniques[num_nuniques!=1].sort_values()\n",
    "mask_nuniques = num_nuniques_nonones<100\n",
    "\n",
    "obj_features = df_obj.columns.tolist()\n",
    "num_features_small = num_nuniques_nonones[mask_nuniques].index.tolist()\n",
    "num_features_large = num_nuniques_nonones[~mask_nuniques].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num features1 ...\n",
      "1) RegistrationYear\n",
      "2) Power\n",
      "3) PostalCode\n",
      "---------------------------------\n",
      "num features2 ...\n",
      "1) LastSeen.quarter\n",
      "2) DateCrawled.month\n",
      "3) DateCrawled.minute\n",
      "4) LastSeen.minute\n",
      "5) DateCrawled.quarter\n",
      "6) LastSeen.month\n",
      "7) DateCreated.year\n",
      "8) DateCrawled.weekofyear\n",
      "9) LastSeen.weekofyear\n",
      "10) DateCrawled.dayofweek\n",
      "11) LastSeen.dayofweek\n",
      "12) DateCreated.month\n",
      "13) Kilometer\n",
      "14) RegistrationMonth\n",
      "15) DateCrawled.hour\n",
      "16) LastSeen.hour\n",
      "17) DateCreated.day\n",
      "18) DateCrawled.day\n",
      "19) LastSeen.day\n",
      "20) DateCrawled.dayofyear\n",
      "21) LastSeen.dayofyear\n",
      "---------------------------------\n",
      "obj features ...\n",
      "1) VehicleType\n",
      "2) Gearbox\n",
      "3) Model\n",
      "4) FuelType\n",
      "5) Brand\n",
      "6) NotRepaired\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('num features1 ...')\n",
    "for i, val in enumerate(num_features_large):\n",
    "    print('{}) {}'.format(i+1, val))\n",
    "print('---------------------------------')  \n",
    "\n",
    "print('num features2 ...')\n",
    "for i, val in enumerate(num_features_small):\n",
    "    print('{}) {}'.format(i+1, val))\n",
    "print('---------------------------------')  \n",
    "\n",
    "print('obj features ...')\n",
    "for i, val in enumerate(obj_features):\n",
    "    print('{}) {}'.format(i+1, val))\n",
    "print('---------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# строки\n",
    "ppl_obj1 = Pipeline([('obj_features', SklearnHelperColumnSelector(obj_features)),\\\n",
    "                     ('label encoder', SklearnHelperLabelEncoder())])\n",
    "\n",
    "ppl_obj2 = Pipeline([('obj_features', SklearnHelperColumnSelector(obj_features)),\\\n",
    "                     ('target encoder', SklearnHelperTargetEncoder(n_iter = 5,\\\n",
    "                                                                   n_folds = 20,\\\n",
    "                                                                   min_samples_leaf = 5,\\\n",
    "                                                                   seed = SEED))])\n",
    "ppl_obj3 = Pipeline([('obj_features', SklearnHelperColumnSelector(obj_features)),\\\n",
    "                     ('impute', SimpleImputer(strategy = 'constant', fill_value = 'other')),\\\n",
    "                     ('one hot encoder', OneHotEncoder(handle_unknown = 'ignore'))])\n",
    "# даты\n",
    "ppl_num_small1 = Pipeline([('num_features_small', SklearnHelperColumnSelector(num_features_small))])\n",
    "ppl_num_small2 = Pipeline([('num_features_small', SklearnHelperColumnSelector(num_features_small)),\\\n",
    "                           ('target encoder', SklearnHelperTargetEncoder(n_iter = 5,\\\n",
    "                                                                         n_folds = 20,\\\n",
    "                                                                         min_samples_leaf = 5,\\\n",
    "                                                                         seed = SEED))])\n",
    "ppl_num_small3 = Pipeline([('num_features_small', SklearnHelperColumnSelector(num_features_small)),\\\n",
    "                           ('one hot encoder', OneHotEncoder(handle_unknown = 'ignore'))])\n",
    "# остальные \n",
    "ppl_num_large1 = Pipeline([('num_features_large', SklearnHelperColumnSelector(num_features_large))])\n",
    "ppl_num_large2 = Pipeline([('num_features_large', SklearnHelperColumnSelector(num_features_large)),\\\n",
    "                           ('target encoder', SklearnHelperTargetEncoder(n_iter = 5,\\\n",
    "                                                                         n_folds = 20,\\\n",
    "                                                                         min_samples_leaf = 5,\\\n",
    "                                                                         seed = SEED))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_tree = Pipeline([('prepare data', \n",
    "                        FeatureUnion([\\\n",
    "                            ('f1',ppl_obj1), ('f2',ppl_obj2),\\\n",
    "                            ('f4',ppl_num_small1), ('f5', ppl_num_small2),\\\n",
    "                            ('f7', ppl_num_large1), ('f8', ppl_num_large2)])),\\\n",
    "                     ('impute', SimpleImputer(strategy = 'constant', fill_value = -1)),\\\n",
    "                     ('scale', MaxAbsScaler()),\\\n",
    "                     ('select features', \n",
    "                        SklearnHelperFeatureSelector(\\\n",
    "                            model = LinearRegression(),\\\n",
    "                            cv = KF,  scoring = NEG_RMSE_SCORER,\\\n",
    "                            show_progress = True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0ea9e8f03d4ee7909f03e76e5ecc8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aabbae3e6a349b1ae4d89455870d1c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=600.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a700337c2b4b9fb98722dd9c007031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d06ff5329f4fe685cd7de155c3e774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2100.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a995b58088a4b418af9ca1baf5f3e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880ab342c0aa4c7197b23175711f3483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa565da0d22f4cfcb70154ed347e3f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b20653ad0b54e87ba50e82276ae1816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best score = -3149.93470\n",
      "new best score = -2667.30648\n",
      "new best score = -2546.60667\n",
      "new best score = -2502.41416\n",
      "new best score = -2502.22558\n",
      "new best score = -2489.90953\n",
      "new best score = -2469.12021\n",
      "new best score = -2467.57538\n",
      "new best score = -2458.75114\n",
      "new best score = -2438.45070\n",
      "new best score = -2430.84819\n",
      "new best score = -2424.41812\n",
      "new best score = -2424.27196\n",
      "new best score = -2403.51065\n",
      "new best score = -2401.63784\n",
      "new best score = -2401.62751\n",
      "new best score = -2401.62463\n",
      "new best score = -2398.77523\n",
      "new best score = -2397.14781\n",
      "new best score = -2397.01701\n",
      "new best score = -2397.01701\n",
      "new best score = -2392.17466\n",
      "new best score = -2392.08914\n",
      "new best score = -2390.25013\n",
      "new best score = -2390.20928\n",
      "new best score = -2390.04600\n",
      "new best score = -2389.85133\n",
      "new best score = -2389.51439\n",
      "new best score = -2389.30152\n",
      "new best score = -2389.18689\n",
      "new best score = -2389.15101\n",
      "new best score = -2388.61163\n",
      "new best score = -2388.61163\n",
      "new best score = -2388.60649\n",
      "new best score = -2388.52025\n",
      "new best score = -2387.27310\n",
      "new best score = -2387.22538\n",
      "new best score = -2387.06755\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7351badc8af64d7fb67a141443bde29c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=22.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best score = -2387.04347\n",
      "new best score = -2387.03545\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f5f95caad54e84acef7f1b4048f54a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best score = -2386.96741\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d0226bc71046cfae6d14cfe29dca8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=19.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('prepare data',\n",
       "                 FeatureUnion(transformer_list=[('f1',\n",
       "                                                 Pipeline(steps=[('obj_features',\n",
       "                                                                  SklearnHelperColumnSelector(columns=['VehicleType',\n",
       "                                                                                                       'Gearbox',\n",
       "                                                                                                       'Model',\n",
       "                                                                                                       'FuelType',\n",
       "                                                                                                       'Brand',\n",
       "                                                                                                       'NotRepaired'])),\n",
       "                                                                 ('label '\n",
       "                                                                  'encoder',\n",
       "                                                                  SklearnHelperLabelEncoder())])),\n",
       "                                                ('f2',\n",
       "                                                 Pipeline(steps=[('obj_features',\n",
       "                                                                  SklearnHelperColumnSelector(columns=['VehicleType',\n",
       "                                                                                                       'Gearb...\n",
       "                                                                  'encoder',\n",
       "                                                                  SklearnHelperTargetEncoder(min_samples_leaf=5,\n",
       "                                                                                             n_folds=20,\n",
       "                                                                                             n_iter=5,\n",
       "                                                                                             seed=13))]))])),\n",
       "                ('impute', SimpleImputer(fill_value=-1, strategy='constant')),\n",
       "                ('scale', MaxAbsScaler()),\n",
       "                ('select features',\n",
       "                 SklearnHelperFeatureSelector(cv=KFold(n_splits=3, random_state=13, shuffle=True),\n",
       "                                              model=LinearRegression(),\n",
       "                                              scoring=make_scorer(neg_rmse_func),\n",
       "                                              show_progress=True))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl_tree.fit(features_tr, target_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_lin = Pipeline([('prepare data', \n",
    "                        FeatureUnion([\\\n",
    "                            ('f1',ppl_obj1), ('f2',ppl_obj2),('f3', ppl_obj3),\\\n",
    "                            ('f4',ppl_num_small1), ('f5', ppl_num_small2),('f6', ppl_num_small3),\\\n",
    "                            ('f7', ppl_num_large1), ('f8', ppl_num_large2)])),\\\n",
    "                     ('impute', SimpleImputer(strategy = 'constant', fill_value = -1)),\\\n",
    "                     ('scale', MaxAbsScaler())])                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb141b86f334819a474ee9f39f1112c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=600.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3422c2dbc6db4b45b70cc74a52c12acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2100.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "904647698b7f412289f3adf4a3e06559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be0005200714e5692f73d8922e83db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=600.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d719eb1c2cb8426a96768e7f417bc2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2100.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9bc2bac77ca4ec1a8b414c2bb94a9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6dc1c3f8fe46069abb442b3d19cbe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=600.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d394277bb7a4958ad266768d4e17dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2100.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0117ffa01821491f89c64efed39b7cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ppl_lin.fit(features_tr, target_tr)\n",
    "X_lin_tr = ppl_tree.transform(features_tr)\n",
    "X_lin_ho = ppl_tree.transform(features_ho)\n",
    "X_lin_te = ppl_tree.transform(features_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr, y_ho, y_te = target_tr.values, target_ho.values, target_te.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5046a4762f0940fb8e9bba1fa12eabdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2227.167073089147 -2249.1033496402915\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "for alpha in tqdm_notebook(np.linspace(.001, .01, 10)):\n",
    "    \n",
    "    ols = LinearRegression()\n",
    "    lasso = Lasso(alpha=alpha, normalize=True)\n",
    "    lasso.fit(X_lin_tr,y_tr)\n",
    "    selected_idxs = lasso.coef_ != 0\n",
    "    X_sel_lin_tr = X_lin_tr[:, selected_idxs]\n",
    "    X_sel_lin_ho = X_lin_ho[:, selected_idxs]\n",
    "    X_sel_lin_te = X_lin_te[:, selected_idxs]\n",
    "    mean_cv_score = cross_val_score(ols,X_sel_lin_tr, y_tr,\\\n",
    "                                     cv = KF, scoring = NEG_RMSE_SCORER).mean()\n",
    "    ols.fit(X_sel_lin_tr, y_tr)\n",
    "    ho_score = neg_rmse_func(y_ho, ols.predict(X_sel_lin_ho))\n",
    "    print(mean_cv_score, ho_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols = LinearRegression()\n",
    "ols.fit(X_lin_tr, y_tr)\n",
    "ho_score = neg_rmse_func(y_ho, ols.predict(X_lin_ho))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2386.980568974011"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(ols,X_lin_tr, y_tr,\\\n",
    "                                     cv = KF, scoring = NEG_RMSE_SCORER).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2412.256430577081"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ho_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_oh_enc = nuniques_c[nuniques_c<100].index.tolist()\n",
    "to_target_enc = nuniques_c[nuniques_c>=100].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_oh_enc = Pipeline([('to_oh_enc', SklearnHelperColumnSelector(to_oh_enc)),\\\n",
    "                       ('one hot encoder', OneHotEncoder(handle_unknown = 'ignore'))])\n",
    "ppl_target_enc = Pipeline([('to_target_enc', SklearnHelperColumnSelector(to_target_enc)),\\\n",
    "                           ('target encoder', SklearnHelperTargetEncoder(n_iter = 10,\\\n",
    "                                                                         n_folds = 20,\\\n",
    "                                                                         min_samples_leaf = 5,\\\n",
    "                                                                         seed = SEED))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56d1a35a1584a45980ba2f8b9327859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292e5f8f9d114b0d8198d09b936b0a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=800.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppl_lin = Pipeline([('prepare data', \n",
    "                        FeatureUnion([('f1',ppl_oh_enc), ('f2',ppl_target_enc)])),\\\n",
    "                     ('scale', MaxAbsScaler())])\n",
    "ppl_lin.fit(features_tr, target_tr)   \n",
    "X_lin_tr = ppl_lin.transform(features_tr)\n",
    "X_lin_ho = ppl_lin.transform(features_ho)\n",
    "X_lin_te = ppl_lin.transform(features_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "ols = LinearRegression()\n",
    "lasso = Lasso(alpha=.005, normalize=True)\n",
    "lasso.fit(X_lin_tr,y_tr)\n",
    "selected_idxs = lasso.coef_ != 0\n",
    "X_sel_lin_tr = X_lin_tr[:, selected_idxs]\n",
    "X_sel_lin_ho = X_lin_ho[:, selected_idxs]\n",
    "X_sel_lin_te = X_lin_te[:, selected_idxs]\n",
    "mean_cv_score = cross_val_score(ols,X_sel_lin_tr, y_tr,\\\n",
    "                                 cv = KF, scoring = NEG_RMSE_SCORER).mean()\n",
    "ols.fit(X_sel_lin_tr, y_tr)\n",
    "ho_score = neg_rmse_func(y_ho, ols.predict(X_sel_lin_ho))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2290.2669597174904, -2315.4838254836673)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mean_cv_score, ho_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# строки\n",
    "str_features = ['VehicleType', 'Gearbox', 'Model', 'FuelType', 'Brand', 'NotRepaired']\n",
    "# даты\n",
    "date_features = ['DateCrawled.year', 'DateCrawled.month', 'DateCrawled.day', 'DateCrawled.dayofweek', 'DateCrawled.hour',\\\n",
    "                 'DateCrawled.minute', 'DateCrawled.dayofyear', 'DateCrawled.weekofyear','DateCrawled.quarter',\\\n",
    "                 'LastSeen.year', 'LastSeen.month','LastSeen.day', 'LastSeen.dayofweek', 'LastSeen.hour','LastSeen.minute',\\\n",
    "                 'LastSeen.dayofyear', 'LastSeen.weekofyear','LastSeen.quarter', 'DateCreated.year', 'DateCreated.month',\n",
    "                 'DateCreated.day', 'RegistrationMonth', 'RegistrationYear']\n",
    "# остальные\n",
    "other_features = ['Power', 'Kilometer', 'PostalCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# строки\n",
    "ppl_str1 = Pipeline([('str', SklearnHelperColumnSelector(str_features)),\\\n",
    "                     ('label encoder', SklearnHelperLabelEncoder())])\n",
    "ppl_str2 = Pipeline([('str', SklearnHelperColumnSelector(str_features)),\\\n",
    "                     ('target encoder', SklearnHelperTargetEncoder(n_iter = 10,\\\n",
    "                                                                   n_folds = 20,\\\n",
    "                                                                   min_samples_leaf = 5,\\\n",
    "                                                                   seed = SEED))])\n",
    "ppl_str3 = Pipeline([('str', SklearnHelperColumnSelector(str_features)),\\\n",
    "                     ('one hot encoder', OneHotEncoder(handle_unknown = 'ignore'))])\n",
    "# даты\n",
    "ppl_date1 = Pipeline([('date', SklearnHelperColumnSelector(date_features))])\n",
    "ppl_date2 = Pipeline([('date', SklearnHelperColumnSelector(date_features)),\\\n",
    "                      ('target encoder', SklearnHelperTargetEncoder(n_iter = 10,\\\n",
    "                                                                   n_folds = 20,\\\n",
    "                                                                   min_samples_leaf = 5,\\\n",
    "                                                                   seed = SEED))])\n",
    "ppl_date3 = Pipeline([('date', SklearnHelperColumnSelector(date_features)),\\\n",
    "                      ('one hot encoder', OneHotEncoder(handle_unknown = 'ignore'))])\n",
    "# остальные \n",
    "ppl_other1 = Pipeline([('other', SklearnHelperColumnSelector(other_features))])\n",
    "ppl_other2 = Pipeline([('other', SklearnHelperColumnSelector(other_features)),\\\n",
    "                       ('target encoder', SklearnHelperTargetEncoder(n_iter = 10,\\\n",
    "                                                                   n_folds = 20,\\\n",
    "                                                                   min_samples_leaf = 5,\\\n",
    "                                                                   seed = SEED))])\n",
    "ppl_other3 = Pipeline([('other', SklearnHelperColumnSelector(other_features)),\\\n",
    "                       ('one hot encoder', OneHotEncoder(handle_unknown = 'ignore'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# бейзлайн\n",
    "baseline_score = np.abs(cross_val_score(DummyRegressor('mean'),\\\n",
    "                                        features_tr, target_tr,\\\n",
    "                                        cv = KF, scoring = NEG_RMSE_SCORER).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_lin = Pipeline([('prepare data', \n",
    "                        FeatureUnion([('f1',ppl_str3), ('f2',ppl_date3), ('f3', ppl_other3)])),\\\n",
    "                     ('scale', MaxAbsScaler())])\n",
    "ppl_lin.fit(features_tr, target_tr)   \n",
    "X_lin_tr = ppl_lin.transform(features_tr)\n",
    "X_lin_ho = ppl_lin.transform(features_ho)\n",
    "X_lin_te = ppl_lin.transform(features_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "ols = LinearRegression()\n",
    "lasso = Lasso(alpha=.005, normalize=True)\n",
    "lasso.fit(X_lin_tr,y_tr)\n",
    "selected_idxs = lasso.coef_ != 0\n",
    "X_sel_lin_tr = X_lin_tr[:, selected_idxs]\n",
    "X_sel_lin_ho = X_lin_ho[:, selected_idxs]\n",
    "X_sel_lin_te = X_lin_te[:, selected_idxs]\n",
    "mean_cv_score = cross_val_score(ols,X_sel_lin_tr, y_tr,\\\n",
    "                                 cv = KF, scoring = NEG_RMSE_SCORER).mean()\n",
    "ols.fit(X_sel_lin_tr, y_tr)\n",
    "ho_score = neg_rmse_func(y_ho, ols.predict(X_sel_lin_ho))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cv_score, ho_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.005,\n",
       " 0.015555555555555555,\n",
       " 0.026111111111111113,\n",
       " 0.03666666666666667,\n",
       " 0.04722222222222222,\n",
       " 0.057777777777777775,\n",
       " 0.06833333333333334,\n",
       " 0.0788888888888889,\n",
       " 0.08944444444444445,\n",
       " 0.1,\n",
       " 0.1,\n",
       " 0.14444444444444446,\n",
       " 0.18888888888888888,\n",
       " 0.23333333333333334,\n",
       " 0.2777777777777778,\n",
       " 0.32222222222222224,\n",
       " 0.3666666666666667,\n",
       " 0.4111111111111111,\n",
       " 0.4555555555555556,\n",
       " 0.5]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_tree = Pipeline([('prepare data', \n",
    "                        FeatureUnion([\\\n",
    "                            ('f1',ppl_str1), ('f2',ppl_str2),\\\n",
    "                            ('f3',ppl_date1), ('f4',ppl_date2),\\\n",
    "                            ('f5', ppl_other1), ('f6', ppl_other2)])),\\\n",
    "                     ('impute', SimpleImputer(strategy = 'constant', fill_value = -1)),\\\n",
    "                     ('scale', MaxAbsScaler()),\\\n",
    "                     ('select features', \n",
    "                        SklearnHelperFeatureSelector(\\\n",
    "                            model = LGBMRegressor(n_jobs=-1, random_state = SEED),\\\n",
    "                            cv = KF,  scoring = NEG_RMSE_SCORER,\\\n",
    "                            show_progress = True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_tree.fit(features_tr, target_tr)\n",
    "X_tree_tr = ppl_tree.transform(features_tr).toarray()\n",
    "X_tree_ho = ppl_tree.transform(features_ho).toarray()\n",
    "X_tree_te = ppl_tree.transform(features_te).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_XXX_lin = {'tr':X_lin_tr, 'ho':X_lin_ho, 'te':X_lin_te}\n",
    "with open('D_XXX_lin.pickle', 'wb') as f:\n",
    "    pickle.dump(D_XXX_lin, f)\n",
    "D_XXX_tree = {'tr':X_tree_tr, 'ho':X_tree_ho, 'te':X_tree_te}\n",
    "with open('D_XXX_tree.pickle', 'wb') as f:\n",
    "    pickle.dump(D_XXX_tree, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('D_XXX_lin.pickle', 'rb') as f:\n",
    "#     D_XXX_lin = pickle.load(f)\n",
    "# X_lin_tr, X_lin_ho, X_lin_te = D_XXX_lin['tr'], D_XXX_lin['ho'], D_XXX_lin['te']\n",
    "\n",
    "# with open('D_XXX_tree.pickle', 'rb') as f:\n",
    "#     D_XXX_tree = pickle.load(f)\n",
    "# X_tree_tr, X_tree_ho, X_tree_te = D_XXX_tree['tr'], D_XXX_tree['ho'], D_XXX_tree['te']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# результаты валидации\n",
    "L_cvAB = []\n",
    "L_best_estimators_names = ['Ridge', 'LinearSVR',\\\n",
    "                           'DecisionTree', 'ExtraTree',\\\n",
    "                           'RandomForest', 'ExtraTrees',\\\n",
    "                           'Lightgbm', 'XGBoost']\n",
    "L_best_estimators = []\n",
    "y_tr, y_ho, y_te = target_tr.values, target_ho.values, target_te.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_best_model = hp_tune_v1(Ridge(), {'alpha':[0, .1, .3, .5, .7, 1, 3, 5, 7, 10, 30, 50, 70]},\\\n",
    "                              X_lin_tr, y_tr, cv=KF, scoring=NEG_RMSE_SCORER)\n",
    "L_best_estimators.append(ridge_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_best_model = hp_tune_v1(LinearSVR(), {'C':[1, 3, 5, 7, 10, 30, 50, 70]},\\\n",
    "                              X_lin_tr, y_tr, cv=KF, scoring=NEG_RMSE_SCORER)\n",
    "L_best_estimators.append(svr_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_pg = {'max_depth':np.arange(7, 41), 'min_samples_leaf':[2, 20, 200]}\n",
    "dt_best_model = hp_tune_v1(DecisionTreeRegressor(), tree_pg, X_tree_tr, y_tr, cv=KF, scoring=NEG_RMSE_SCORER)\n",
    "L_best_estimators.append(dt_best_model)\n",
    "\n",
    "exdt_best_model = hp_tune_v1(ExtraTreeRegressor(), tree_pg, X_tree_tr, y_tr, cv=KF, scoring=NEG_RMSE_SCORER)\n",
    "L_best_estimators.append(exdt_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees_pg = {'max_depth':np.arange(5, 21),'min_samples_leaf':[2, 20],'n_estimators':[10], 'n_jobs':[-1], 'random_state':[SEED]}\n",
    "\n",
    "rf_best_model = hp_tune_v1(RandomForestRegressor(),trees_pg, X_tree_tr, y_tr, cv=KF, scoring=NEG_RMSE_SCORER)\n",
    "bp = {'n_estimators':100}\n",
    "bp.update(**rf_best_model.get_params())\n",
    "rf_best_model.set_params(**bp)\n",
    "L_best_estimators.append(rf_best_model)\n",
    "\n",
    "exts_best_model = hp_tune_v1(ExtraTreesRegressor(),trees_pg, X_tree_tr, y_tr, cv=KF, scoring=NEG_RMSE_SCORER)\n",
    "bp = {'n_estimators':100}\n",
    "bp.update(**exts_best_model.get_params())\n",
    "exts_best_model.set_params(**bp)\n",
    "L_best_estimators.append(exts_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_grid1 = {'n_estimators':[10], 'n_jobs':[-1], 'random_state':[SEED],\\\n",
    "             'max_depth':np.arange(2, 21).tolist(),\\\n",
    "             'num_leaves':[4, 8, 16, 32, 64, 128, 256, 512, 1024, 1200, 1500],\\\n",
    "             'min_child_samples':[20, 50]}\n",
    "lgb_grid2 = {'subsample':np.linspace(.1, 1, 10),\\\n",
    "             'colsample_bytree':np.linspace(.1, 1, 10)}\n",
    "lgb_grid3 = {'learning_rate':np.linspace(.01, .1, 50), 'n_estimators':[100]}\n",
    "\n",
    "lgb_best_model = hp_tune_v2(LGBMRegressor(),\\\n",
    "                            lgb_grid1, lgb_grid2, lgb_grid3,\\\n",
    "                            X_tree_tr, y_tr, X_tree_ho, y_ho,\\\n",
    "                            cv=KF, scoring=NEG_RMSE_SCORER)\n",
    "L_best_estimators.append(lgb_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid1 = {'n_estimators':[10], 'n_jobs':[-1], 'random_state':[SEED],\\\n",
    "             'max_depth':[14, 15, 16, 17, 18],\\\n",
    "             'min_child_weight':[20]}\n",
    "xgb_grid2 = {'subsample':[.5, .6, .7, .8, .9, 1],\\\n",
    "             'colsample_bytree':[.5, .6, .7, .8, .9, 1]}\n",
    "xgb_grid3 = {'learning_rate':[.04, .045, .05,.055, .06, .065], 'n_estimators':[100]}\n",
    "\n",
    "xgb_best_model = hp_tune_v2(XGBRegressor(),\\\n",
    "                            xgb_grid1, xgb_grid2, xgb_grid3,\\\n",
    "                            X_tree_tr, y_tr, X_tree_ho, y_ho,\\\n",
    "                            cv=KF, scoring=NEG_RMSE_SCORER)\n",
    "L_best_estimators.append(xgb_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('L_best_estimators.pickle', 'wb') as f:\n",
    "#     pickle.dump(L_best_estimators, f)\n",
    "# with open('L_best_estimators.pickle', 'rb') as f:\n",
    "#     L_best_estimators = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_base_X_tr = (X_lin_tr, X_lin_tr, X_tree_tr, X_tree_tr, X_tree_tr, X_tree_tr, X_tree_tr, X_tree_tr)\n",
    "L_base_X_ho = (X_lin_ho, X_lin_ho, X_tree_ho, X_tree_ho, X_tree_ho, X_tree_ho, X_tree_ho, X_tree_ho)\n",
    "L_base_X_te = (X_lin_te, X_lin_te, X_tree_te, X_tree_te, X_tree_te, X_tree_te, X_tree_te, X_tree_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_cvAB = []\n",
    "for name, estimator, X_tr, X_ho in tqdm_notebook(zip(L_best_estimators_names, L_best_estimators, L_base_X_tr, L_base_X_ho),\\\n",
    "                                                 total = len(L_base_X_tr)):\n",
    "    start = time.time()\n",
    "    mean_cv_score = cross_val_score(estimator, X_tr, y_tr, cv = KF, scoring = NEG_RMSE_SCORER, n_jobs=-1).mean()\n",
    "    end = time.time()\n",
    "    duration = round(end-start)\n",
    "    \n",
    "    estimator.fit(X_tr, y_tr)\n",
    "    ho_score = neg_rmse_func(y_ho, estimator.predict(X_ho))\n",
    "    \n",
    "    L_cvAB.append((name, mean_cv_score, ho_score, duration))\n",
    "    \n",
    "cvAB = pd.DataFrame(L_cvAB, columns = ['model', 'cv', 'ho', 'duration']).set_index('model').astype(float).abs()\n",
    "cvAB = cvAB.loc[cvAB[['cv', 'ho']].mean(1).sort_values().index]\n",
    "cvAB.to_pickle('cvAB.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cvAB = pd.read_pickle('cvAB.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnHelperMetaFeaturesRegressor(BaseEstimator, TransformerMixin):    \n",
    "    def __init__(self, model, cv, path_to_folder):\n",
    "        self.model = model\n",
    "        self.cv =cv\n",
    "        self.path_to_folder = path_to_folder\n",
    "    def fit(self, X, y=None):\n",
    "        model = self.model\n",
    "        Z = np.zeros((X.shape[0], 1)\n",
    "        for i, (tr_idx, val_idx) in enumerate(self.cv.split(y)):            \n",
    "            model.fit(X[tr_idx], y[tr_idx])\n",
    "            Z[val_idx, 0] = model.predict(X[val_idx]))\n",
    "        \n",
    "            if not os.path.exists(self.path_to_folder):\n",
    "                os.makedirs(self.path_to_folder)\n",
    "            else:\n",
    "                shutil.rmtree(self.path_to_folder, ignore_errors=True)\n",
    "                os.makedirs(self.path_to_folder)\n",
    "            path_fitted = os.path.join([self.path_to_folder, f'_fitted{i}.pickle'])\n",
    "            with open(path_fitted, 'wb') as f:\n",
    "                pickle.dump(model, f)   \n",
    "        self.Z = Z\n",
    "        self.X = X\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        if np.all(self.X == np.array(X)):\n",
    "            return self.Z\n",
    "        else:\n",
    "            L = []\n",
    "            for filename in os.listdir(self.path_to_folder):\n",
    "                with open(filename, 'rb') as f:\n",
    "                    model = pickle.load(f)\n",
    "                    L.append(model.predict(X).flatten().reshape(-1,1))\n",
    "            return np.mean(np.c_[L], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnHelperMetaFeaturesRegressor_v2(BaseEstimator, TransformerMixin):    \n",
    "    def __init__(self, model, cv, n_iterations, path_to_folder1, path_to_folder2):\n",
    "        self.model = model\n",
    "        self.cv =cv\n",
    "        self.n_iterations = n_iterations\n",
    "        self.path_to_folder1 = path_to_folder1\n",
    "        self.path_to_folder2 = path_to_folder2\n",
    "    def fit(self, X, y=None):\n",
    "        model = self.model\n",
    "        subsamples = [.1, .2, .3, .4, .5, .6, .7, .8, .9]\n",
    "        for j in tqdm_notebook(range(self.n_iterations)):\n",
    "            np.random.seed(j)\n",
    "            subsample = np.random.choice(subsamples)\n",
    "            feat_use = np.random.choice(np.arange(X.shape[1]), np.int32(np.around(subsample*X.shape[1])), replace = False) \n",
    "            Z = np.zeros((X.shape[0], 1)\n",
    "            for i, (tr_idx, val_idx) in enumerate(self.cv.split(y)):            \n",
    "                model.fit(X[tr_idx, feat_use], y[tr_idx])\n",
    "                Z[val_idx, 0] = model.predict(X[val_idx, feat_use]))\n",
    "\n",
    "                if not os.path.exists(self.path_to_folder1):\n",
    "                    os.makedirs(self.path_to_folder1)                    \n",
    "                else:\n",
    "                    shutil.rmtree(self.path_to_folder1, ignore_errors=True)\n",
    "                    os.makedirs(self.path_to_folder1)\n",
    "                \n",
    "                if not os.path.exists(self.path_to_folder2):\n",
    "                    os.makedirs(self.path_to_folder2)                    \n",
    "                else:\n",
    "                    shutil.rmtree(self.path_to_folder2, ignore_errors=True)\n",
    "                    os.makedirs(self.path_to_folder2)\n",
    "                \n",
    "                path_fitted = os.path.join([self.path_to_folder1, f'_fitted{j}.{i}.pickle'])\n",
    "                path_feat_idxs = os.path.join([self.path_to_folder2, f'_fitted{j}.{i}.pickle'])                \n",
    "                \n",
    "                with open(path_fitted, 'wb') as f:\n",
    "                    pickle.dump(model, f)\n",
    "                with open(path_feat_idxs, 'wb') as f:\n",
    "                    pickle.dump(feat_use, f)                    \n",
    "        self.Z = Z\n",
    "        self.X = X\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        if np.all(self.X == np.array(X)):\n",
    "            return self.Z\n",
    "        else:\n",
    "            Ls=[]\n",
    "            i = 0\n",
    "            for filename1, filename2 in zip(os.listdir(self.path_to_folder1), os.listdir(self.path_to_folder2)):\n",
    "                L = []\n",
    "                with open(filename1, 'rb') as f:\n",
    "                    model = pickle.load(f)\n",
    "                with open(filename2, 'rb') as f:\n",
    "                    use_idxs = pickle.load(f)\n",
    "                    L.append(model.predict(X[:, use_idxs]).flatten().reshape(-1,1))\n",
    "                i+=1\n",
    "                if (i % self.cv.get_n_splits())==0:\n",
    "                    Ls.append(np.mean(np.c_[L], 1)) \n",
    "            return np.c_[Ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
